clear all
close all
clc

%simulate data from a gaussian mixture

mu = [2; 0.1];
sigma = cat(3,0.5,0.5);
p = ones(1,2)/2;
gm = gmdistribution(mu,sigma,p);

gmPDF = @(x) arrayfun( @(x0) pdf(gm,x0), x);


rng('default') % For reproducibility
D = random(gm,300);


figure(1)
plot(linspace(-3,3),gmPDF(linspace(-3,3)),'LineWidth',2);
hold on
histogram(D,10,'Normalization','pdf');
title('Gaussian mixture pdf');
hold off
t_star=0.5;
u=@(D) exp(-D*t_star);

D_obs=0.8;
u_obs=u(D_obs)+normrnd(0,0.1);

X_data=u(D)+normrnd(0,0.1,[300,1]);
figure(2)
plot(X_data,D,'bo')
hold on
plot(u_obs,D_obs,'r*')
%% lets try the mdn to fit the model


% Set up network parameters.
nin = 1;			% Number of inputs.
nhidden = 100;			% Number of hidden units.
ncentres = 2;			% Number of mixture components.
dim_target = 1;			% Dimension of target space
mdntype = '0';			% Currently unused: reserved for future use
alpha = 100;			% Inverse variance for weight initialisation
				% Make variance small for good starting point

% Create and initialize network weight vector.
net = mdn(nin, nhidden, ncentres, dim_target, mdntype);
init_options = zeros(1, 18);
init_options(1) = -1;	% Suppress all messages
init_options(14) = 10;  % 10 iterations of K means in gmminit
net = mdninit(net, alpha, X_data, init_options);

options = zeros(1,18);
options(1) = 1;			% This provides display of error values.
options(14) = 200;		% Number of training cycles. 


disp('We initialise the neural network model, which is an MLP with a')
disp('Gaussian mixture model with three components and spherical variance')
disp('as the error function.  This enables us to model the complete')
disp('conditional density function.')
disp(' ')
disp('Next we train the model for 200 epochs using a scaled conjugate gradient')
disp('optimizer.  The error function is the negative log likelihood of the')
disp('training data.')
disp(' ')
disp('Press any key to continue.')
pause

% Train using scaled conjugate gradients.
[net, options] = netopt(net, options,X_data,D, 'scg');

plotvals = linspace(0,3)';
mixes = mdn2gmm(mdnfwd(net, plotvals));


p2 = plot(X_data, D, '--y');
hold on
y = zeros(1, length(plotvals));
priors = zeros(length(plotvals), ncentres);
c = zeros(length(plotvals), 2);
widths = zeros(length(plotvals), ncentres);
for i = 1:length(plotvals)
  [m, j] = max(mixes(i).priors);
  y(i) = mixes(i).centres(j,:);
  c(i,:) = mixes(i).centres';
end
p3 = plot(plotvals, y, '*r');
legend([p2 p3], 'function', 'MDN mode');
hold off

%%
clc
disp('We can also plot how the mixture model parameters depend on x.')
disp('First we plot the mixture centres, then the priors and finally')
disp('the variances.')
disp(' ')
disp('Press any key to continue.')
pause
%%fh2 = figure;
subplot(3, 1, 1)
plot(plotvals, c)
hold on
title('Mixture centres')
legend('centre 1', 'centre 2')
hold off

priors = reshape([mixes.priors], mixes(1).ncentres, size(mixes, 2))';
%%fh3 = figure;
subplot(3, 1, 2)
plot(plotvals, priors)
hold on
title('Mixture priors')
legend('centre 1', 'centre 2', 'centre 3')
hold off

variances = reshape([mixes.covars], mixes(1).ncentres, size(mixes, 2))';
%%fh4 = figure;
subplot(3, 1, 3)
plot(plotvals, variances)
hold on
title('Mixture variances')
legend('centre 1', 'centre 2')
hold off

disp('The last figure is a contour plot of the conditional probability')
disp('density generated by the Mixture Density Network.  Note how it')
disp('is well matched to the regions of high data density.')
disp(' ')
disp('Press any key to continue.')
%%
out_mix = mdn2gmm(mdnfwd(net, u_obs));
mu_obs=out_mix.centres;
prior_obs=out_mix.priors;
sigma_obs=cat(3,out_mix.covars(1),out_mix.covars(2));

gm_obs = gmdistribution(mu_obs,sigma_obs,prior_obs);

gmPDF_obs = @(x) arrayfun( @(x0) pdf(gm_obs,x0), x);
figure(1)
plot(linspace(-3,3),gmPDF_obs(linspace(-3,3)),'LineWidth',2);
hold on
xline(D_obs)

%%
rng('default') % For reproducibility
D = random(gm_obs,300);


figure(1)
plot(linspace(-3,3),gmPDF_obs(linspace(-3,3)),'LineWidth',2);
hold on
histogram(D,10,'Normalization','pdf');
title('Gaussian mixture pdf');
hold off


X_data=u(D)+normrnd(0,0.1,[300,1]);
figure(2)
plot(X_data,D,'bo')
hold on
plot(u_obs,D_obs,'r*')
%%
% Set up network parameters.
nin = 1;			% Number of inputs.
nhidden = 100;			% Number of hidden units.
ncentres = 2;			% Number of mixture components.
dim_target = 1;			% Dimension of target space
mdntype = '0';			% Currently unused: reserved for future use
alpha = 100;			% Inverse variance for weight initialisation
				% Make variance small for good starting point

% Create and initialize network weight vector.
net = mdn(nin, nhidden, ncentres, dim_target, mdntype);
init_options = zeros(1, 18);
init_options(1) = -1;	% Suppress all messages
init_options(14) = 10;  % 10 iterations of K means in gmminit
net = mdninit(net, alpha, X_data, init_options);

options = zeros(1,18);
options(1) = 1;			% This provides display of error values.
options(14) = 200;		% Number of training cycles. 


disp('We initialise the neural network model, which is an MLP with a')
disp('Gaussian mixture model with three components and spherical variance')
disp('as the error function.  This enables us to model the complete')
disp('conditional density function.')
disp(' ')
disp('Next we train the model for 200 epochs using a scaled conjugate gradient')
disp('optimizer.  The error function is the negative log likelihood of the')
disp('training data.')
disp(' ')
disp('Press any key to continue.')
pause

% Train using scaled conjugate gradients.
[net, options] = netopt(net, options,X_data,D, 'scg');

plotvals = linspace(0,3)';
mixes = mdn2gmm(mdnfwd(net, plotvals));


p2 = plot(X_data, D, '--y');
hold on
y = zeros(1, length(plotvals));
priors = zeros(length(plotvals), ncentres);
c = zeros(length(plotvals), 2);
widths = zeros(length(plotvals), ncentres);
for i = 1:length(plotvals)
  [m, j] = max(mixes(i).priors);
  y(i) = mixes(i).centres(j,:);
  c(i,:) = mixes(i).centres';
end
p3 = plot(plotvals, y, '*r');
legend([ p2 p3], 'function', 'MDN mode');
hold off

%%
clc
disp('We can also plot how the mixture model parameters depend on x.')
disp('First we plot the mixture centres, then the priors and finally')
disp('the variances.')
disp(' ')
disp('Press any key to continue.')
pause
fh2 = figure;
subplot(3, 1, 1)
plot(plotvals, c)
hold on
title('Mixture centres')
legend('centre 1', 'centre 2')
hold off

priors = reshape([mixes.priors], mixes(1).ncentres, size(mixes, 2))';
%%fh3 = figure;
subplot(3, 1, 2)
plot(plotvals, priors)
hold on
title('Mixture priors')
legend('centre 1', 'centre 2', 'centre 3')
hold off

variances = reshape([mixes.covars], mixes(1).ncentres, size(mixes, 2))';
%%fh4 = figure;
subplot(3, 1, 3)
plot(plotvals, variances)
hold on
title('Mixture variances')
legend('centre 1', 'centre 2')
hold off

disp('The last figure is a contour plot of the conditional probability')
disp('density generated by the Mixture Density Network.  Note how it')
disp('is well matched to the regions of high data density.')
disp(' ')
disp('Press any key to continue.')
%%

out_mix_2 = mdn2gmm(mdnfwd(net, u_obs));
mu_obs_2=out_mix_2.centres;
prior_obs_2=out_mix_2.priors;
sigma_obs_2=cat(3,out_mix_2.covars(1),out_mix_2.covars(2));

gm_obs_2 = gmdistribution(mu_obs_2,sigma_obs_2,prior_obs_2);

gmPDF_obs_2 = @(x) arrayfun( @(x0) pdf(gm_obs_2,x0), x);
figure(4)
plot(linspace(-3,3),gmPDF_obs_2(linspace(-3,3)),'LineWidth',2);
hold on
xline(D_obs)

%%
rng('default') % For reproducibility
D = random(gm_obs_2,300);


figure(1)
plot(linspace(-3,3),gmPDF_obs_2(linspace(-3,3)),'LineWidth',2);
hold on
histogram(D,10,'Normalization','pdf');
title('Gaussian mixture pdf');
hold off


X_data=u(D)+normrnd(0,0.1,[300,1]);
figure(2)
plot(X_data,D,'bo')
hold on
plot(u_obs,D_obs,'r*')
%%
% Set up network parameters.
nin = 1;			% Number of inputs.
nhidden = 100;			% Number of hidden units.
ncentres = 2;			% Number of mixture components.
dim_target = 1;			% Dimension of target space
mdntype = '0';			% Currently unused: reserved for future use
alpha = 100;			% Inverse variance for weight initialisation
				% Make variance small for good starting point

% Create and initialize network weight vector.
net = mdn(nin, nhidden, ncentres, dim_target, mdntype);
init_options = zeros(1, 18);
init_options(1) = -1;	% Suppress all messages
init_options(14) = 10;  % 10 iterations of K means in gmminit
net = mdninit(net, alpha, X_data, init_options);

options = zeros(1,18);
options(1) = 1;			% This provides display of error values.
options(14) = 200;		% Number of training cycles. 


disp('We initialise the neural network model, which is an MLP with a')
disp('Gaussian mixture model with three components and spherical variance')
disp('as the error function.  This enables us to model the complete')
disp('conditional density function.')
disp(' ')
disp('Next we train the model for 200 epochs using a scaled conjugate gradient')
disp('optimizer.  The error function is the negative log likelihood of the')
disp('training data.')
disp(' ')
disp('Press any key to continue.')
pause

% Train using scaled conjugate gradients.
[net, options] = netopt(net, options,X_data,D, 'scg');

plotvals = linspace(0,3)';
mixes = mdn2gmm(mdnfwd(net, plotvals));


p2 = plot(X_data, D, '--y');
hold on
y = zeros(1, length(plotvals));
priors = zeros(length(plotvals), ncentres);
c = zeros(length(plotvals), 2);
widths = zeros(length(plotvals), ncentres);
for i = 1:length(plotvals)
  [m, j] = max(mixes(i).priors);
  y(i) = mixes(i).centres(j,:);
  c(i,:) = mixes(i).centres';
end
p3 = plot(plotvals, y, '*r');
legend([ p2 p3], 'function', 'MDN mode');
hold off

%%
clc
disp('We can also plot how the mixture model parameters depend on x.')
disp('First we plot the mixture centres, then the priors and finally')
disp('the variances.')
disp(' ')
disp('Press any key to continue.')
pause
fh2 = figure;
subplot(3, 1, 1)
plot(plotvals, c)
hold on
title('Mixture centres')
legend('centre 1', 'centre 2')
hold off

priors = reshape([mixes.priors], mixes(1).ncentres, size(mixes, 2))';
%%fh3 = figure;
subplot(3, 1, 2)
plot(plotvals, priors)
hold on
title('Mixture priors')
legend('centre 1', 'centre 2', 'centre 3')
hold off

variances = reshape([mixes.covars], mixes(1).ncentres, size(mixes, 2))';
%%fh4 = figure;
subplot(3, 1, 3)
plot(plotvals, variances)
hold on
title('Mixture variances')
legend('centre 1', 'centre 2')
hold off

disp('The last figure is a contour plot of the conditional probability')
disp('density generated by the Mixture Density Network.  Note how it')
disp('is well matched to the regions of high data density.')
disp(' ')
disp('Press any key to continue.')
%%

out_mix_2 = mdn2gmm(mdnfwd(net, u_obs));
mu_obs_2=out_mix_2.centres;
prior_obs_2=out_mix_2.priors;
sigma_obs_2=cat(3,out_mix_2.covars(1),out_mix_2.covars(2));

gm_obs_2 = gmdistribution(mu_obs_2,sigma_obs_2,prior_obs_2);

gmPDF_obs_2 = @(x) arrayfun( @(x0) pdf(gm_obs_2,x0), x);
figure(4)
plot(linspace(-3,3),gmPDF_obs_2(linspace(-3,3)),'LineWidth',2);
hold on
xline(D_obs)
